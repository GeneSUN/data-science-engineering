{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM25bnhushCC4XWF1HULq3P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeneSUN/data-science-engineering/blob/main/trip_duration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcyCQCl4NDh3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Distributional (probabilistic) regression demo for \"trip duration (days)\"\n",
        "Goal: estimate P(duration > 30 | x), including for a NEW destination market (never seen before)\n",
        "\n",
        "Key idea:\n",
        "- Train a model that predicts a full conditional distribution p(y|x), not just E[y|x]\n",
        "- Then compute exceedance probability via the CDF: P(Y>30)=1-F(30)\n",
        "\n",
        "This script uses NGBoost (Natural Gradient Boosting for probabilistic prediction),\n",
        "which predicts distribution parameters (e.g., LogNormal mean/scale) directly. :contentReference[oaicite:0]{index=0}\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Synthesize tabular trip data\n",
        "# -----------------------------\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "N = 25000\n",
        "n_origins = 60\n",
        "n_destinations_seen = 140\n",
        "n_destinations_new = 10\n",
        "\n",
        "# \"Market embeddings\" (latent factors) for origin/destination\n",
        "# These are *not* one-hot IDs; think of them as market features (geo, infra, ops, etc.)\n",
        "k = 4\n",
        "origin_emb = rng.normal(size=(n_origins, k))\n",
        "dest_emb_seen = rng.normal(size=(n_destinations_seen, k))\n",
        "dest_emb_new  = rng.normal(loc=0.5, scale=1.2, size=(n_destinations_new, k))  # shifted to simulate new market differences\n",
        "\n",
        "# Create rows\n",
        "origin_id = rng.integers(0, n_origins, size=N)\n",
        "dest_id   = rng.integers(0, n_destinations_seen, size=N)\n",
        "\n",
        "# Trip-level features (distance, weight, customs flag, seasonality, carrier reliability, etc.)\n",
        "distance_km = rng.gamma(shape=2.2, scale=500.0, size=N)          # long tail distances\n",
        "weight_kg   = rng.lognormal(mean=2.0, sigma=0.6, size=N)         # skewed\n",
        "is_customs  = rng.binomial(1, 0.18, size=N)                      # cross-border/customs\n",
        "peak_season = rng.binomial(1, 0.25, size=N)                      # seasonal congestion\n",
        "carrier_rel = np.clip(rng.normal(loc=0.0, scale=1.0, size=N), -2, 2)  # higher is better\n",
        "\n",
        "# Market features via embeddings\n",
        "O = origin_emb[origin_id]                # (N,k)\n",
        "D = dest_emb_seen[dest_id]               # (N,k)\n",
        "\n",
        "# A latent \"difficulty\" score; new markets later will shift this\n",
        "difficulty = (\n",
        "    0.0020 * distance_km\n",
        "    + 0.15  * np.log1p(weight_kg)\n",
        "    + 0.55  * is_customs\n",
        "    + 0.25  * peak_season\n",
        "    - 0.20  * carrier_rel\n",
        "    + 0.30  * (O[:,0] - D[:,0])\n",
        "    + 0.15  * (O[:,1] * D[:,1])\n",
        ")\n",
        "\n",
        "# Convert to positive duration days using log-normal data generating process\n",
        "# y ~ LogNormal(mu, sigma) with heteroscedastic sigma depending on features\n",
        "mu_true = 2.2 + 0.35 * np.tanh(difficulty)          # controls typical duration\n",
        "sigma_true = 0.35 + 0.10 * (is_customs + peak_season) + 0.05 * (distance_km > 1500)\n",
        "\n",
        "y_days = rng.lognormal(mean=mu_true, sigma=sigma_true)\n",
        "\n",
        "# Assemble training dataframe (NO raw IDs needed for generalization; keep embeddings + numeric features)\n",
        "X = pd.DataFrame({\n",
        "    \"distance_km\": distance_km,\n",
        "    \"log_weight\": np.log1p(weight_kg),\n",
        "    \"is_customs\": is_customs,\n",
        "    \"peak_season\": peak_season,\n",
        "    \"carrier_rel\": carrier_rel,\n",
        "})\n",
        "for j in range(k):\n",
        "    X[f\"origin_f{j}\"] = O[:, j]\n",
        "    X[f\"dest_f{j}\"]   = D[:, j]\n",
        "\n",
        "y = y_days\n",
        "\n",
        "# Train/valid split\n",
        "idx = rng.permutation(N)\n",
        "train_idx, val_idx = idx[: int(0.8*N)], idx[int(0.8*N):]\n",
        "X_train, y_train = X.iloc[train_idx], y[train_idx]\n",
        "X_val,   y_val   = X.iloc[val_idx],   y[val_idx]\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Fit a distributional regression model (NGBoost)\n",
        "# -----------------------------\n",
        "# NGBoost predicts parameters of a chosen distribution p(y|x) and trains with proper scoring rules\n",
        "# like Negative Log Likelihood. :contentReference[oaicite:1]{index=1}\n",
        "from ngboost import NGBRegressor\n",
        "from ngboost.distns import LogNormal\n",
        "\n",
        "ngb = NGBRegressor(\n",
        "    Dist=LogNormal,          # good for positive, skewed durations\n",
        "    n_estimators=800,\n",
        "    learning_rate=0.03,\n",
        "    minibatch_frac=0.7,\n",
        "    verbose=False,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "ngb.fit(X_train, y_train)\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Exceedance probability: P(Y > 30 | x)\n",
        "# -----------------------------\n",
        "THRESH = 30.0\n",
        "\n",
        "# NGBoost returns a distribution object per row with methods like cdf/survival.\n",
        "# survival(y) = P(Y > y). (If survival not present, use 1 - cdf.)\n",
        "pred_dist_val = ngb.pred_dist(X_val)\n",
        "\n",
        "# Robustly compute exceedance\n",
        "try:\n",
        "    p_exceed_val = pred_dist_val.survival(THRESH)\n",
        "except Exception:\n",
        "    p_exceed_val = 1.0 - pred_dist_val.cdf(THRESH)\n",
        "\n",
        "print(\"Example exceedance probabilities (validation):\")\n",
        "print(p_exceed_val[:10])\n",
        "\n",
        "# -----------------------------\n",
        "# 4) \"New market\" inference: destination never seen in training\n",
        "# -----------------------------\n",
        "# Key: you can only generalize to unseen destinations if you have destination FEATURES\n",
        "# (geo, infrastructure, lead-time SLA, customs complexity, etc.), not only an ID.\n",
        "# Here we simulate new destinations via new embeddings dest_emb_new.\n",
        "M = 5  # make 5 hypothetical shipments to new markets\n",
        "origin_id_new = rng.integers(0, n_origins, size=M)\n",
        "dest_id_new   = rng.integers(0, n_destinations_new, size=M)\n",
        "\n",
        "O2 = origin_emb[origin_id_new]\n",
        "D2 = dest_emb_new[dest_id_new]\n",
        "\n",
        "X_new = pd.DataFrame({\n",
        "    \"distance_km\": rng.gamma(shape=2.2, scale=600.0, size=M),\n",
        "    \"log_weight\": np.log1p(rng.lognormal(mean=2.1, sigma=0.7, size=M)),\n",
        "    \"is_customs\": rng.binomial(1, 0.30, size=M),     # maybe higher for new markets\n",
        "    \"peak_season\": rng.binomial(1, 0.25, size=M),\n",
        "    \"carrier_rel\": np.clip(rng.normal(0.0, 1.0, size=M), -2, 2),\n",
        "})\n",
        "for j in range(k):\n",
        "    X_new[f\"origin_f{j}\"] = O2[:, j]\n",
        "    X_new[f\"dest_f{j}\"]   = D2[:, j]\n",
        "\n",
        "pred_dist_new = ngb.pred_dist(X_new)\n",
        "try:\n",
        "    p_exceed_new = pred_dist_new.survival(THRESH)\n",
        "except Exception:\n",
        "    p_exceed_new = 1.0 - pred_dist_new.cdf(THRESH)\n",
        "\n",
        "print(\"\\nNew-market exceedance probabilities P(Y>30 | x):\")\n",
        "for i in range(M):\n",
        "    print(f\"trip{i}:  P(days>30)={float(p_exceed_new[i]):.3f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Notes for real-world \"new market\" (cold start)\n",
        "# -----------------------------\n",
        "# If your destination is truly unseen, avoid one-hot IDs.\n",
        "# Use:\n",
        "# - destination geo features, distance, lanes, customs, carrier network, port capacity, SLA tiers\n",
        "# - embeddings learned from metadata (text, category, route graph)\n",
        "# - hierarchical / partial pooling to borrow strength across markets (Bayesian / multilevel) :contentReference[oaicite:2]{index=2}\n",
        "#\n",
        "# For deep learning, you can do the same concept with torch.distributions + NLL: :contentReference[oaicite:3]{index=3}\n"
      ]
    }
  ]
}