# Contextual Embeddings for Tabular Data: TabTransformer

- https://arxiv.org/pdf/2012.06678

## Context-free vs Contextual Embedding

### classical context-free embedding

<img width="1388" height="968" alt="image" src="https://github.com/user-attachments/assets/f72326ac-3257-4aa8-b58b-a38f5d1ab76f" />


For a categorical feature such as food type (e.g., Pizza, Pasta):
```
Embedding(device_type) ────▶ [fixed vector]

Pizza  → [ 0.12, -0.87,  1.34]
Pasta  → [-0.44,  0.23,  0.91]
```

Each category is mapped to a single, fixed embedding vector. This embedding remains the same regardless of context.
The meaning of “Pizza” is assumed to be identical everywhere, it does not take into account:
- which region the item appears in
- which style or topping is associated
- which other features are present in the same row

### Contextual Embedding


<img width="1400" height="774" alt="image" src="https://github.com/user-attachments/assets/0145fb59-cde9-4668-8acb-1ab6d1918915" />

In contrast, contextual embeddings are generated by allowing categorical features to interact with one another through self-attention.

```
(region) ─┐
(style)  ─┼──▶ Attention ──▶ Pizza (contextual vector)
(price)  ─┘
```



**interpretability**

> We investigate the effectiveness and interpretability of the resulting contextual embeddings generated by the Transformers. We find that highly correlated features (including feature pairs in the same column and cross column) result in embedding vectors that are close together in Euclidean distance, whereas no such pattern exists in contextfree embeddings learned in a baseline MLP model.



<img width="1181" height="580" alt="image" src="https://github.com/user-attachments/assets/8e961625-80e2-496c-981f-6e6a17df0e2e" />

## Example:

```python
from pytorch_tabular.models import TabTransformerConfig
from pytorch_tabular.config import DataConfig, TrainerConfig, OptimizerConfig

data_config = DataConfig(
trainer_config = TrainerConfig(
model_config = TabTransformerConfig(
    task="classification",
    # Embedding / column embedding (paper-style)
    input_embed_dim=input_embed_dim,
    share_embedding=True,
    share_embedding_strategy="fraction",
    shared_embedding_fraction=0.25,


tabular_model = TabularModel(
    data_config=data_config,
    model_config=model_config,
    optimizer_config=optimizer_config,
    trainer_config=trainer_config,
)
```

<img width="419" height="1027" alt="Untitled" src="https://github.com/user-attachments/assets/a02791b7-cecf-4bef-9f59-446eda059391" />


In this [notebook](https://colab.research.google.com/drive/1novgNUYQWIUNkhD_5mFx9XzhtJ2ebeiW#scrollTo=Build_TabTransformer_in_PyTorch_Tabular), I compare three approaches for handling categorical features.
The results show that CatBoost and TabTransformer both perform very well, and are significantly better than the traditional one-hot encoding approach.

Additionally, TabTransformer demonstrates greater robustness to noise and missing values, which is particularly important in real-world tabular datasets.

<img width="561" height="105" alt="Screenshot 2026-01-15 at 4 57 15 PM" src="https://github.com/user-attachments/assets/7272f93c-972e-4409-b58e-67a534dcb73d" />



In addition to PyTorch Tabular, another implementation of TabTransformer is also available:
- https://github.com/aruberts/TabTransformerTF/blob/main/build/lib/tabtransformertf/models/tabtransformer.py

## Self-supervised Learning

For a scenario, when there are a few labeled examples and a large number of unlabeled examples, we introduce a pre-training procedure to train the Transformer layers using unlabeled data. This is followed by fine-tuning of the pre-trained Transformer layers along with the top MLP layer using the labeled data.



TabTransformer is superior to both a baseline MLP and recent deep networks for tabular data while matching the performance of tree-based ensemble models (GBDT).

3. We demonstrate the robustness of TabTransformer against **noisy and missing** data.
4. We provide and extensively study a **two-phase pretraining then fine-tune procedure** for tabular data, beating the state-of-the-art performance of semi-supervised learning methods.

https://chatgpt.com/s/t_696835b75ec081918a41fadf3bce6cec



```
[E_region, E_device, E_plan]
        ↓
Multi-Head Self Attention
        ↓
Contextualized embeddings

```

```
[x1] [x2] [region] [device] [plan]
  ↓    ↓      ↓        ↓        ↓
 tokenized feature embeddings
                ↓
           Transformer
                ↓
             prediction

```
